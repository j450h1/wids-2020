---
title: "Understanding Word Embeddings"
author: "Julia Silge"
date: '2020-03-13'
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: ["default", "xaringan-themer.css", "css/footer_plus.css"]
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
    seal: false  
    includes:
      in_header: header.html  
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
library(knitr)
knitr::opts_chunk$set(cache = TRUE, warning = FALSE, 
                      message = FALSE, dpi = 180, cache.lazy = FALSE)
library(tidyverse)
library(silgelib)
theme_set(theme_plex())
conflicted::conflict_prefer("filter", "dplyr")
```

```{r include=FALSE, eval=FALSE}
library(tidyverse)

complaints <- read_csv("complaints.csv.zip") %>%
  janitor::clean_names() %>%
  filter(date_received >= "2019-01-01",
         !is.na(consumer_complaint_narrative)) %>%
  select(complaint_id, date_received, product, issue, company, state,
         consumer_complaint_narrative)

write_csv(complaints, "complaints.csv.gz")
```


```{r xaringan-themer, include = FALSE}
library(xaringanthemer)

mono_accent(
  base_color           = "#132240",
  header_font_google   = google_font("Trirong", "700"),
  text_font_google     = google_font("Source Sans Pro"),
  code_font_google     = google_font("Droid Mono"),
  text_font_size       = "30px",
  code_font_size       = "20px",
  footnote_font_size   = "18px" 
)
```

layout: true

<div class="my-footer"><span>bit.ly/silge-wids-2020</span></div> 

---

class: inverse, left, middle

background-image: url(figs/patrick-fore-0gkw_9fy0eQ-unsplash.jpg)
background-size: cover

# Understanding 
# Word 
# Embeddings

### Julia Silge | 13 March 2020

---

class: inverse, left, bottom

background-image: url(figs/patrick-fore-0gkw_9fy0eQ-unsplash.jpg)
background-size: cover

# Find me at...

<a href="http://twitter.com/juliasilge" style="color: white;"><i class="fa fa-twitter fa-fw"></i>&nbsp; @juliasilge</a><br>
<a href="http://github.com/juliasilge" style="color: white;"><i class="fa fa-github fa-fw"></i>&nbsp; @juliasilge</a><br>
<a href="https://juliasilge.com" style="color: white;"><i class="fa fa-link fa-fw"></i>&nbsp; juliasilge.com</a><br>
<a href="https://tidytextmining.com" style="color: white;"><i class="fa fa-book fa-fw"></i>&nbsp; tidytextmining.com</a><br>
<a href="mailto:julia.silge@gmail.com" style="color: white;"><i class="fa fa-paper-plane fa-fw"></i>&nbsp; julia.silge@gmail.com</a>

---

class: inverse, center, middle

# `r emo::ji("bookmark_tabs")` TEXT AS DATA `r emo::ji("bar_chart")`

---

# Text as data

Let's look at complaints submitted to the [United States Consumer Financial Protection Bureau (CFPB)](https://www.consumerfinance.gov/data-research/consumer-complaints/).

```{r}
library(tidyverse)

complaints <- read_csv("complaints.csv.gz")
names(complaints)
```

---

# Text as data

```{r}
complaints %>%
  sample_n(10) %>%
  pull(consumer_complaint_narrative)
```

---

# Text as data

What is a typical way to represent this text data for modeling?

```{r}
library(tidytext)
library(SnowballC)

complaints %>% 
  unnest_tokens(word, consumer_complaint_narrative) %>% 
  anti_join(get_stopwords()) %>%  
  mutate(stem = wordStem(word)) %>% 
  count(complaint_id, stem) %>%  
  bind_tf_idf(stem, complaint_id, n) %>% 
  cast_dfm(complaint_id, stem, tf_idf)
```


---

class: inverse, left, bottom

background-image: url(figs/patrick-fore-0gkw_9fy0eQ-unsplash.jpg)
background-size: cover

# This representation...

- .large[is incredibly sparse]
- .large[of high dimensionality]
- .large[with a huge number of features]

---

class: inverse, center, middle

# `r emo::ji("page_facing_up")` WORD EMBEDDINGS `r emo::ji("notebook_with_decorative_cover")`

---

class: right, middle

<h1 class="fa fa-quote-left fa-fw"></h1>

<h1> You shall know a word by the company it keeps. </h1>

<h1 class="fa fa-quote-right fa-fw"></h1>

.large[John Rupert Firth]

---

class: inverse

# Modern word embeddings

--

- word2vec

--

- GloVe

--

- fastText

--

- language models with transformers like ULMFiT and ELMo

---

class: inverse, left, top

background-image: url(figs/patrick-fore-0gkw_9fy0eQ-unsplash.jpg)
background-size: cover

# We can determine word embeddings using...

- .large[word counts]
- .large[matrix factorization]

.footnote[
<a href="https://multithreaded.stitchfix.com/blog/2017/10/18/stop-using-word2vec/" style="color: white">Moody, Chris. "Stop using word2vec." MultiThreaded blog (2017).</a>
]

---

# Counting words

First, we tokenize and transform this dataset to a [tidy data structure](https://www.tidytextmining.com/), then create nested dataframes.

```{r eval=FALSE}
tidy_complaints <- complaints %>%
  select(complaint_id, consumer_complaint_narrative) %>%
  unnest_tokens(word, consumer_complaint_narrative) %>%
  group_by(word) %>%
  filter(n() >= 50) %>%
  ungroup()

nested_words <- tidy_complaints %>%
  nest(words = c(word))
```

```{r echo=FALSE}
tidy_complaints <- complaints %>%
  select(complaint_id, consumer_complaint_narrative) %>%
  unnest_tokens(word, consumer_complaint_narrative) %>%
  group_by(word) %>%
  dplyr::filter(n() >= 50) %>%
  ungroup()

nested_words <- tidy_complaints %>%
  nest(words = c(word))
```

---

class: inverse

# Sliding window size? `r emo::ji("thinking")`

--

- Determines semantic meaning the embeddings capture

--

- Smaller window size (3-4) focuses on how the word is used and learns what other words are functionally similar

--

- Larger window size (~10) captures the domain or topic of each word

---

class: inverse

# Point-wise mutual information

--

- How often do words occur on their own?

--

- How often words occur together with other words?

--

- PMI is a measure of association to compute this

--

- PMI is logarithm of the probability of finding two words together, normalized for the probability of finding each of the words alone



---

# Calculate PMI

We use PMI to measure which words occur together more often than expected based on how often they occurred on their own.

```{r echo=FALSE}
slide_windows <- function(tbl, window_size) {
  
  skipgrams <- slider::slide(tbl, ~.x, .after = window_size - 1, .step = 1, .complete = TRUE)
  
  safe_mutate <- safely(mutate)
  
  out <- map2(skipgrams, 1:length(skipgrams), ~ safe_mutate(.x, window_id = .y))
  
  out %>%
    transpose() %>% 
    pluck("result") %>% 
    compact() %>%
    bind_rows()
}
```


```{r}
library(widyr)
library(furrr)

plan(multiprocess)  ## for parallel processing

tidy_pmi <- nested_words %>%  
  mutate(words = future_map(words, slide_windows, 4)) %>%
  unnest(words) %>%
  unite(window_id, complaint_id, window_id) %>%
  pairwise_pmi(word, window_id)
```

---

# Calculate PMI

When PMI is high, the two words are associated with each other, likely to occur together.

```{r}
tidy_pmi
```


---

# Time for word vectors! `r emo::ji("tada")`

We determine word vectors using singular value decomposition.

```{r}
tidy_word_vectors <- tidy_pmi %>%
  widely_svd(
    item1, item2, pmi, 
    nv = 100, maxit = 1000
  )
```

---

class: inverse, left, bottom

### Each word can be represented as a numeric vector in this 

- .large[new,]
- .large[dense,]
- .large[100-dimensional] 

### feature space. 

Which words are close to each other in this new feature space of word embeddings?

```{r echo=FALSE}
nearest_neighbors <- function(df, token) {
  df %>%
    widely(~ . %*% (.[token, ]), sort = TRUE, maximum_size = NULL)(item1, dimension, value) %>%
    select(-item2)
}
```

---

# Explore CFPB word embeddings

```{r}
tidy_word_vectors %>%
  nearest_neighbors("error")
```

---

# Explore CFPB word embeddings

```{r}
tidy_word_vectors %>%
  nearest_neighbors("month")
```

---

# Explore CFPB word embeddings

```{r}
tidy_word_vectors %>%
  nearest_neighbors("fee")
```

---

# Explore CFPB word embeddings

```{r eval=FALSE}
tidy_word_vectors %>%
  filter(dimension <= 8) %>%
  group_by(dimension) %>%
  top_n(12, abs(value)) %>%
  ungroup %>%
  ggplot(aes(item1, value, fill = as.factor(dimension))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~dimension, scales = "free_y", ncol = 4) +
  coord_flip()
```

---

```{r echo=FALSE, fig.height=4.2}
tidy_word_vectors %>%
  dplyr::filter(dimension <= 8) %>%
  group_by(dimension) %>%
  top_n(12, abs(value)) %>%
  ungroup %>%
  mutate(item1 = reorder_within(item1, value, dimension)) %>%
  ggplot(aes(item1, value, fill = as.factor(dimension))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~dimension, scales = "free_y", ncol = 4) +
  scale_x_reordered() +
  coord_flip() +
  theme(axis.text.x = element_blank()) +
  labs(x = NULL, y = NULL)
```

---

```{r echo=FALSE, fig.height=4.2}
tidy_word_vectors %>%
  dplyr::filter(dimension > 8,
                dimension <= 16) %>%
  group_by(dimension) %>%
  top_n(12, abs(value)) %>%
  ungroup %>%
  mutate(item1 = reorder_within(item1, value, dimension)) %>%
  ggplot(aes(item1, value, fill = as.factor(dimension))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~dimension, scales = "free_y", ncol = 4) +
  scale_x_reordered() +
  coord_flip() +
  theme(axis.text.x = element_blank()) +
  labs(x = NULL, y = NULL)
```

---

# Embeddings in modeling

The classic and simplest approach is to treat each document as a collection of words and summarize the word embeddings into **document embeddings**.

```{r}
word_matrix <- tidy_complaints %>% 
  count(complaint_id, word) %>%  
  cast_sparse(complaint_id, word, n)

embedding_matrix <- tidy_word_vectors %>%
  cast_sparse(item1, dimension, value)

doc_matrix <- word_matrix %*% embedding_matrix

dim(doc_matrix)
```

---

class: inverse, center, middle

### `r emo::ji("sob")` WHAT IF YOUR DATASET IS TOO SMALL? `r emo::ji("weary")`

---

# Try pre-trained word embeddings

```{r}
library(textdata)

glove6b <- embedding_glove6b(dimensions = 100)
```

---

# Try pre-trained word embeddings

```{r}
glove6b
```

```{r, echo=FALSE}
tidy_glove <- glove6b %>%
  pivot_longer(contains("d"),
               names_to = "dimension") %>%
  rename(item1 = token)
```

---

# Explore GloVe word embeddings

```{r}
tidy_glove %>%
  nearest_neighbors("error")
```

---

# Explore GloVe word embeddings

```{r}
tidy_glove %>%
  nearest_neighbors("month")
```

---

# Explore GloVe word embeddings


```{r}
tidy_glove %>%
  nearest_neighbors("fee")
```

---

class: inverse, left, bottom

background-image: url(figs/patrick-fore-0gkw_9fy0eQ-unsplash.jpg)
background-size: cover

## Pre-trained word embeddings...

- encode rich semantic relationships

- can be less than ideal for specific tasks

---

class: inverse, left, bottom

background-image: url(figs/patrick-fore-0gkw_9fy0eQ-unsplash.jpg)
background-size: cover

# Fairness and 
# Word 
# Embeddings 

---

# Fairness and word embeddings 

--

- Embeddings are trained or learned from a large corpus of text data

--

- Human prejudice or bias in the corpus becomes imprinted into the embeddings

---

class: inverse

# Fairness and word embeddings 

--

- African American first names are associated with more unpleasant feelings than European American first names

--

- Women's first names are more associated with family and men's first names are more associated with career

--

- Terms associated with women are more associated with the arts and terms associated with men are more associated with science


.footnote[
<a href="https://arxiv.org/abs/1608.07187" style="color: white">Caliskan, Bryson, and Narayanan. "Semantics Derived Automatically from Language Corpora Contain Human-Like Biases." Science 356.6334 (2017): 183–186.</a>
]

---

```{r echo=FALSE, fig.align="center"}
knitr::include_graphics("figs/turkish.png")
```

---

class: inverse, middle, center

## Bias is so ingrained in word embeddings that they can be used to quantify change in social attitudes over time

.footnote[
<a href="https://www.pnas.org/content/115/16/E3635" style="color: white">Garg, Nikhil, et al. "Word embeddings quantify 100 years of gender and ethnic stereotypes." Proceedings of the National Academy of Sciences 115.16 (2018): E3635-E3644.</a>
]

---

# Biased training data 

--

- Embeddings are trained or learned from a large corpus of text data

--

- For example, consider the case of Wikipedia

--

- Wikipedia both reflects social/historical biases **and** generates bias


.footnote[
[Wagner, Claudia, et al. "Women through the glass ceiling: gender asymmetries in Wikipedia." EPJ Data Science 5.1 (2016): 5.](https://link.springer.com/article/10.1140/epjds/s13688-016-0066-4)
]

---

# Biased embeddings in models

Consider a straightforward sentiment analysis model trained to predict how positive text is. **Compare:**

.pull-left[
"Let's go get Italian food!" `r emo::ji("blush")`

]

.pull-right[
"Let's go get Mexican food!" `r emo::ji("confused")`
]


.footnote[
[Speer, Robyn. "How to make a racist AI without really trying." ConceptNet blog (2017).](http://blog.conceptnet.io/posts/2017/how-to-make-a-racist-ai-without-really-trying/)
]
---

class: inverse, left, top

background-image: url(figs/patrick-fore-0gkw_9fy0eQ-unsplash.jpg)
background-size: cover

# Consider some options

--

- .large[Find your own embeddings]

--

- .large[Consider not using embeddings]

--

- .large[Can embeddings be debiased?]
---

class: inverse

# Can embeddings be debiased?

--

- Embeddings can be reprojected to mitigate a specific bias (such as gender bias) using specific sets of words

--

- Training data can be augmented with counterfactuals

--

- Other researchers suggest that fairness corrections occur at a decision

--

- Evidence indicates that debiasing still allows stereotypes to seep back in


.footnote[
<a href="https://arxiv.org/abs/1903.03862" style="color: white">Gonen, Hila, and Yoav Goldberg. "Lipstick on a pig: Debiasing methods cover up systematic gender biases in word embeddings but do not remove them." arXiv preprint arXiv:1903.03862 (2019).</a>
]

---

class: inverse, left, bottom

background-image: url(figs/patrick-fore-0gkw_9fy0eQ-unsplash.jpg)
background-size: cover

## Word embeddings in the
# REAL WORLD

---

class: inverse, left

background-image: url(figs/patrick-fore-0gkw_9fy0eQ-unsplash.jpg)
background-size: cover

# Thanks!

<a href="http://twitter.com/juliasilge" style="color: white;"><i class="fa fa-twitter fa-fw"></i>&nbsp; @juliasilge</a><br>
<a href="http://github.com/juliasilge" style="color: white;"><i class="fa fa-github fa-fw"></i>&nbsp; @juliasilge</a><br>
<a href="https://juliasilge.com" style="color: white;"><i class="fa fa-link fa-fw"></i>&nbsp; juliasilge.com</a><br>
<a href="https://tidytextmining.com" style="color: white;"><i class="fa fa-book fa-fw"></i>&nbsp; tidytextmining.com</a><br>
<a href="mailto:julia.silge@gmail.com" style="color: white;"><i class="fa fa-paper-plane fa-fw"></i>&nbsp; julia.silge@gmail.com</a>

Slides created with <a href="http://remarkjs.com/" style="color: #5A6B8C;"><b>remark.js</b></a> and <a href="https://github.com/yihui/xaringan" style="color: #5A6B8C;"><b>xaringan</b></a>

Photo by <a href="https://unsplash.com/@patrickian4?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText" style="color: #5A6B8C;"><b>Patrick Fore</b></a> on <a href="https://unsplash.com/s/photos/letters?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText" style="color: #5A6B8C;"><b>Unsplash</b></a>



